\documentclass{article}
\usepackage[final]{nips_2016} % produce camera-ready copy
% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% % to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

% \usepackage{nips_2016}

% % to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{mathtools}
\title{Deep Learning Assignment 3}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.
\author{
  Peter Yun-shao Sung
  \texttt{yss265@nyu.edu} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle
\section{General Questions}
\paragraph{(a)} Say if the first module is:
\begin{equation}
max(W_1X)
\end{equation}
where the W input layer maybe doing summation and summation just like matrix mutiplication does $WX$, and the $max$ function is a non-linear active function modifying the valuse like a neuron does before entering the next module:
\begin{equation}
W_2(max(W_2X))
\end{equation}
If now we don't have the active function then the formula will looks like:
\begin{equation}
W_2(W_1X) \to \bar{W}X
\end{equation}
which eventually all $W_i$ can become a single module $\bar{W}$

\paragraph{(b)} For dictionary learning using sparse coding:
\begin{equation}
\min_{D, \alpha} {1\over n} \displaystyle\sum_{i=1}^n({1\over 2}||x_i-D\alpha||^2 + \lambda||\alpha_i||_1)
\end{equation}
Which is a joint minimization problem with respect to dictionary $D$ and $\alpha$\\
And for autoencoders:
\begin{equation}
\min_{W_{de}, W_{en}} {1 \over n} \displaystyle\sum_{i=1}^n||W_{de}\sigma(W_{en}x_i)-x_i||^2 \\
\end{equation}
Where $\sigma$ is some non-linear function (e.g. shrinkage). Similar to dictionary learning, autoencoders is also a joint optimization problem respect to encoder and decoder matrix $W_{en}$, $W_{de}$. Indeed we can make the $\alpha$ to become $\sigma(W_{en}x)$ and this will make them very similar. However, there are two main factors making them different: One is autoencoder does not has the term for regulator, and therefore sparsity is not encouraged. Another one is autoencoder uses the model to find the code, while sparse coding approaching it by means of the optimizations.

\section{Softmax regression gradient calculation}
Given
\begin{equation}
\hat{y} = \sigma (Wx+b) \textbf{ , where $x \in \mathbb{R}^d$,$W \in \mathbb{R}^{k\times d}$, $b \in \mathbb{R}^k$}
\end{equation}
where d is the input dimension, k is the number of classes, $\sigma$ is the softmax function:
\begin{equation}
\sigma(a)_i = {exp(a_i) \over \sum_j exp(a_j)}
\end{equation}
Which means a given input $x$ will output $y$ with probability of each class

\paragraph{(a)} Derive ${\partial l\over \partial W_{ij}}$\\
If the given cross-entropy loss defined as followed:
\begin{equation}
l(y, \hat{y}) = -\sum_i y_i\log\hat{y_i}
\end{equation}
As $W_{ij}$ will affect the prediction of class $i$ by multipling index $j$ in $x$, therefore we can derive:
\begin{equation}
{\partial l\over \partial W_{ij}} = {\partial l \over \partial \hat{y_i}} {\partial \hat{y_i} \over \partial W_{ij}}
\end{equation}
where:
\begin{equation}
l(y, \hat{y}) = -\sum_i y_i\log\hat{y_i} = -(y_i\log\hat{y_1} + y_2\log\hat{y_2} + \dots + y_i\log\hat{y_i} + \dots)
\end{equation}
and therefore
\begin{equation}
{\partial l \over \partial \hat{y_i}} = {-y_i\over \hat{y_i}}
\end{equation}
And we can rewrite for only for $\hat y_i$:
\begin{equation}
\hat{y_i} = {exp(a_i) \over \Sigma_j exp(a_j)} = {exp(a_i) \over C + exp(a_i)} \textbf{,  where $C = \sum_{k\neq i} exp(a_k)$}
\end{equation}
Since
\begin{equation}
{\partial exp(a_i) \over \partial W_{ij}} = X_jexp(a_i)
% {\partial exp(a_i) \over \partial W_{ij}} = W_{ij}exp(a_i)
\end{equation}
Therefore
\begin{equation}
{\partial \hat{y_i} \over \partial W_{ij}} = X_j\hat{y_i}(1-\hat{y_i})
% {\partial \hat{y_i} \over \partial W_{ij}} = W_{ij}\hat{y_i}(1-\hat{y_i})
\end{equation}
% \begin{equation}
% \hat{y_i} = {e^{(W_{ij}X_j+b_i)} \over C + e^{(W_{ij}X_j+b_i)} } \textbf{, where $C = \sum_{k\neq j} e^{W_{ik}X_k+b_i}$}
% \end{equation}
% \begin{equation}
% {\partial \hat{y_i} \over \partial W_{ij}} = {X_je^{(W_{ij}X_j+b_i)} \over C + e^{(W_{ij}X_j+b_i)} } - {X_je^{2(W_{ij}X_j+b_i)} \over (C + e^{(W_{ij}X_j+b_i)})^2} = X_j \hat{y_i} (1- \hat{y_i})
% \end{equation}
Finally, we will get the result of ${\partial l\over \partial W_{ij}}$:
\begin{equation}
{\partial l\over \partial W_{ij}} = {\partial l \over \partial \hat{y_i}} {\partial \hat{y_i} \over \partial W_{ij}} = -X_jy_i(1- \hat{y_i})
\end{equation}
\paragraph{(b)} What happen when $y_{c_1}=1, \hat{y}_{c_2}=1, c_1 \neq c_2$\\
This means something like $y = [1, 0, 0]^T$ and $\hat{y} = [0,0,1]^T$, and the predict is far different from true lable. This will cause the log part in loss (3) become negative infinity. We may not need to worry this because before one of the class predicted close to 1 and everything else close to 0, it will generate a great positive loss the the class that is miss-predicted trying to make the predict right to true label.
\section{Chain rule}
Without explicitly deriving the formula of $f(x, y)$, can we apply layers of functions to represent function $f$, which is similar to build deep learning architecture.
\begin{equation}
\begin{aligned}
f &= {{x^2+\sigma (y)}\over 3x + y - \sigma(x)} = {a \over b} \\
\implies {\partial f \over \partial x} &= {\partial a \over \partial x}{1\over b} - {a\over b^2}{\partial b \over \partial x} \\
\implies {\partial f \over \partial y} &= {\partial a \over \partial y}{1\over b} - {a\over b^2}{\partial b \over \partial y} \\
\implies {\partial a \over \partial x} &= 2x \\
\implies {\partial a \over \partial y} &= \sigma(y)(1-\sigma(y)) \\
\implies {\partial b \over \partial x} &= 3-\sigma(x)(1-\sigma(x)) \\
\implies {\partial b \over \partial y} &= 1 \\
\end{aligned}
\end{equation}

\paragraph{(b)} As $x = 1$ and $y = 0$, then for each of value from the function listed above:
\begin{equation}
\begin{aligned}
a &= 1+ \sigma(0) = 1.5 \\
b &= 3+ 0+ \sigma(1) = 2.269 \\
{\partial a \over \partial x} &= 2\cdot 1 = 2 \\
{\partial a \over \partial y} &= 0.5(1-0.5) = 0.25 \\
{\partial b \over \partial x} &= 3-0.731(1-0.731) = 2.803\\
{\partial b \over \partial y} &= 1 \\
\end{aligned}
\end{equation}
Therefore, applying each of the gradient at $(x,y) = (1,0)$ to the chain rule, we will get:

\begin{equation}
\begin{aligned}
{\partial f \over \partial x} &= {\partial a \over \partial x}{1\over b} - {a\over b^2}{\partial b \over \partial x} = 2\cdot{1\over2.269} - {1.5\over(2.269)^2}\cdot2.803 = 0.0647 \\
{\partial f \over \partial y} &= {\partial a \over \partial y}{1\over b} - {a\over b^2}{\partial b \over \partial y} =  0.25\cdot{1\over2.269} - {1.5 \over(2.269)^2}\cdot 1 =  -0.1811\\
\end{aligned}
\end{equation}

\section{Variants of pooling}
\paragraph{(a)} The purpose of pooling is to progressively reducing the spatial size to reduce the amount of parametersm and thereforealso to control the issue of overfitting. There are many different varients of pooling for example max-pooling, average pooling, and fractional max-pooling, and they can be found in torch as function $SpatialMaxPooling$, $SpatialAveragePooling$, and $SpatialLPPooling$.\\
\paragraph{(b)} For $SpatialMaxPooling$ the definition is as followed:
\begin{equation}
\begin{matrix}
x_{out} = \max(x_i^{(in)}) \quad \text{ for signals in pool region}
\end{matrix}
\end{equation}

For $SpatialAveragePooling$ the definition is as followed:
\begin{equation}
\begin{matrix}
x_{out} = {1\over n}\displaystyle\sum_i^nx_i^{(in)} \quad \text{ for signals in pool region}
\end{matrix}
\end{equation}

For $SpatialLPPooling$ the definition is as followed:
\begin{equation}
\begin{matrix}
x_{out} = {1\over n}(\displaystyle\sum_i^n(x_i^{(in)})^p)^{1\over p} \quad \text{ for signals in pool region}
\end{matrix}
\end{equation}
\paragraph{(c)} Max-pooling is very useful as it helps to eliminate non-maximinal values and reduce the amount of parameter. However, if we just do max-pooling, the performance is limited due to its rapid reduction of spatial size, and the disjoint nature of the pooling region. Therefore, LP-pooling, which is an biologically inspired mehtod, will an moderate method that can reduce the spatial size as well as keeping the signal meaning in the pooling region.

\section{Convolution}
\paragraph{(a)} As it is using 3x3 kernal along x and y axis of input, which is 5 and 5 respectively. The output of this layer will be $(5-3+1)\times(5-3+1)$ which is 3x3.\\
\paragraph{(b)} Assuming the kernel operation is point-point multiplication and summation, then the output of this layer is:\\
$\begin{pmatrix}
  109 & 92 & 72 \\[0.4em]
  108 & 85 & 74 \\[0.4em]
  110 & 74 & 79
\end{pmatrix}$ \\
\paragraph{(c)} \\
$\begin{pmatrix}
  4 & 7 & 10 & 6 & 3 \\[0.4em]
  9 & 17 & 25 & 16 & 8 \\[0.4em]
  11 & 23 & 34 & 23 & 11 \\[0.4em]
  7 & 16 & 24 & 17 & 8 \\[0.4em]
  2 & 6 & 9 &7 & 3
\end{pmatrix}$ \\


\section{Optimization}
\paragraph{(a)} say the encoder and decoder is defined as:
\begin{equation}
\begin{align}
z &= \hat{W_1}x \\[0.4em]
\tilde{x} &= \hat{W_2}\sigma(z)  \quad \text{say $\sigma$ is a sigmoid function}
\end{align}
\end{equation}
And therefore the reconstruction loss $J$ will be:
\begin{equation}
J(\hat{W_1}, \hat{W_2}) = (\tilde{x}-x)^2 = (\hat{W_2}\sigma(\hat{W_1}x)-x)^2
\end{equation}

\paragraph{(b)} To have the gradient of reconstruction loss respective to the parameters, we take the derivative of each parameters:
\begin{equation}
\begin{align}
{\partial J \over \partial \hat{W_1}} &= 2(\hat{W_2}\sigma(\hat{W_1}x)-x) \cdot \hat{W_2}\sigma(\hat{W_1}x)(1-\sigma(\hat{W_1}x))x \\[0.4em]
% {\partial \sigma(\hat{W_1}x)\over \partial \hat{W_1}} =  \\[0.4em]
{\partial J \over \partial \hat{W_2}} &= 2(\hat{W_2}\sigma(\hat{W_1}x)-x) \cdot \sigma(\hat{W_1}x)
\end{align}
\end{equation}

\paragraph{(c)} Say now we are at stage $t$ and would like to compute $W_1^{t+1}$ and $W_2^{t+1}$:
\begin{equation}
\begin{align}
\hat{W_1}^{t+1} &= \hat{W_1}^t - \mu_1^t {\partial J \over \partial \hat{W_1}^t} = \hat{W_1}^t - \mu_1^t( 2(\hat{W_2}\sigma(\hat{W_1}x)-x) \cdot \hat{W_2}\sigma(\hat{W_1}x)(1-\sigma(\hat{W_1}x))x ) \\[0.4em]
\hat{W_2}^{t+1} &= \hat{W_2}^t - \mu_2^t {\partial J \over \partial \hat{W_2}^t} = \hat{W_2}^t - \mu_2^t( 2(\hat{W_2}\sigma(\hat{W_1}x)-x) )
\end{align}
\end{equation}
where $\mu_1^t$ and $\mu_2^t$ are the step size at stage t

\paragraph{(d)} The updates during stochastic gradient descent usually involves Move-Forward and Correction stages and this oscillation may delay the efficiency of convergence, and therefore adding a momentum term may make the update toward the good direction as well as with the previous update history considered:
\begin{equation}
\begin{matrix}
\hat{W_1}^{t+1} = \hat{W_1}^t - \mu_1^t {\partial J \over \partial \hat{W_1}^t} + \Delta \hat{W_1}^t \\[0.4em]
\hat{W_2}^{t+1} = \hat{W_2}^t - \mu_2^t {\partial J \over \partial \hat{W_2}^t} + \Delta \hat{W_2}^t
\end{matrix}
\end{equation}

\section{Top-k error}
For image classification, sometime the class is ambiguous, and the loss during is being modified to consider multiple label. The top-k error rate is the fraction of test images for which the correct label is not among the top-k labels considered most probable. The reason why ImageNet using both top-5 and top-1 is due to sometimes only looking at top-1 error cannot be objective enought to evaluate the model because the image itself contains multi-label, and therefore evaluating top-5 error is important too.

\section{t-SNE}
\paragraph{(a)} Supose we have two-dimensional map that embedded within a space with much higher dimensionality, and if in this high dimension each data points are mutually equaldistant, and the mapping is not able to faithfully performed if the convergence is simply just based on mutual distance. Therefore, the crowding problem is defined as followed:

\textit{The area of the two-dimensional map that is available to accommodate moderately distant datapoints will not be nearly large enough compared with the area available to accommodate nearby datapoints.}

The approach of t-SNE to alleviate this problem is interesting. It converts distances into probability by Gaussian distribution. Then , in low-dimension space, it uses a probability distribution that has much heavier tails than Gaussian to conver distance to probability. After solving the joint optimization problem, this model provides the conversion that data with small distances in high-dimension space can get converted with much larger distances. Moreover, it eliminates the concerns of unwanted attractive forces between dissimilar points.

There were attempts to solve the crowding problem, for example, by adding a small repulsion force to all springs between data points. However, the optimization of this method is tedious, and also after optimization it happend offently that two parts of cluster get seperated but then there is no force to pull them back together.
\paragraph{(b)} Derive ${\partial C \over \partial y_i}$\\
Let's define some variable for convenience:
\begin{equation}
\begin{aligned}
d_{ij} &= ||y_i-y_j|| \\[0.4em]
Z &= \displaystyle\sum_{k \neq l} exp(-d_{kl}^2)
\end{aligned}
\end{equation}
As we may notice that if we change $y_i$ plus its symmetric properity, $d_{ij}$ and $d_{ji}$ will be affected for $\forall j$. Therefore the gradient of C respect to $y_i$ is given by:
\begin{equation}
\begin{aligned}
{\partial C \over \partial y_i} &= \displaystyle\sum_{j} ( {\partial C \over \partial d_{ij}} + {\partial C \over \partial d_{ji}} ) (y_i - y_j) \\[0.4em]
&= 2 \displaystyle\sum_{j} {\partial C \over \partial d_{ij}}(y_i - y_j)
\end{aligned}
\end{equation}
Thus, the next question is to derive ${\partial C \over \partial d_{ij}}$:
\begin{equation}
\begin{aligned}
C &= \displaystyle\sum_{i} \sum_{j}(p_{ij}\log p_{ij} - p_{ij}\log q_{ij}) \\[0.4em]
{\partial C \over \partial d_{ij}} &= - \displaystyle\sum_{k \neq l}p_{kl}{\partial(\log q_{kl})\over \partial d_{ij}} = - \displaystyle\sum_{k \neq l}p_{kl}{\partial(\log q_{kl}Z - \log Z)\over \partial d_{ij}} \\[0.4em]
&= - \displaystyle\sum_{k \neq l} p_{kl}({1\over q_{ij}Z} {\partial (\exp(-d_{kl}^2)) \over \partial d_{ij} } - {1\over Z}{\partial Z \over \partial d_{ij}} ) \\[0.4em]
\end{aligned}
\end{equation}
Gradient ${\partial (\exp(-d_{kl}^2)) \over \partial d_{ij} }$ is only nonzero when $k = i$ and $l = j$. Therefore we can rewrite the formula above to:
\begin{equation}
\begin{aligned}
{\partial C \over \partial d_{ij}} &= {p_{ij}\over q_{ij}Z}(2 \exp(-d_{ij}^2)) - \displaystyle\sum_{k \neq l} {2\exp (-d_{ij}^2)\over Z}\\[0.4em]
&= 2p_{ij} - 2q_{ij}
\end{aligned}
\end{equation}
Therefore, the combine to the previous formula we will get:
\begin{equation}
\begin{aligned}
{\partial C \over \partial y_i} &= 2 \displaystyle\sum_{j} {\partial C \over \partial d_{ij}}(y_i - y_j) \\[0.4em]
&= 4 \displaystyle\sum_{j}(p_{ij}-q_{ij}) (y_i -y_j)

\end{aligned}
\end{equation}


\section{Proximal gradient descent}
\paragraph{(a)} Since Proximal operator is defined as:
\begin{equation}
prox_{h,t}(x) = argmin_z{1\over2}||z-x||_2^2 + th(z)
\end{equation}
which the optimal condition is to have the gradient w.r.t $z$ equal to 0:
\begin{equation}
0 \in z-x+t\partial h(z)
\end{equation}
if function $h(z)=||z||_1$ and $z_i \neq 0$, then:
\begin{equation}
\partial h(z) = sign(z)
\end{equation}
And therefore the optimal solution $z^\ast$ will be:
\begin{equation}
z^\ast = x - t\cdot sign(z^\ast)
\end{equation}
Noted that if $z_i^\ast<0$, then $x_i<-t$, and if $z_i^\ast>0$, then $x_i>t$. This implies $|x_i|>t$ and $sign(z_i^\ast) = sign(x_i)$, and we can rewrite formula to:
\begin{equation}
z_i^\ast = x_i - t\cdot sign(x_i)
\end{equation}
Then if the solution $z_i^\ast=0$, the subgradient of l1-norm is in the interval of [-1, 1], and we can write:
\begin{equation}
0 \in -x_i + t \cdot [-1,1] \implies x_i \in [-t, t] \implies |x_i| \leq t
\end{equation}
Therefore the solution of Proximal operator will be:
\begin{equation}
z_i^\ast = \begin{cases}
    0       & \quad \text{if } |x_i|\leq t\\
    x_i - t \cdot sign(x_i)  & \quad \text{if } |x_i|>t\\
  \end{cases}
\end{equation}
which is
\begin{equation}
prox_{h,t}(x) = S_t(x) = (|x|-t)_+\odot sign(x) \quad \text{(element-wise)}
\end{equation}
which is a soft-threshold fuction with t as threshold value\\
\paragraph{(b)} In the field of signal processing, the true signal usually will be blurred as followed:
\begin{equation}
Ax = b
\end{equation}
where $A$ is the blur operation, b is the known observed blured-signal. The way to solve true signal $x$ is called deblurring problem:
\begin{equation}
min_x\{F(x) \equiv {1\over 2}||b-Ax||_2^2 + \lambda||x||_1\}
\end{equation}
This is ISTA problem, and as we can see the first term is convex and differentiable, and the second term is convex and simple l1-norm function. Then the ISTA is become one example of proximal gradient descent\\
\paragraph{(c)} From the definition of Proximal operator the optimal solution is where ${\partial prox_{h,t}\over \partial z} = 0$, and therefore we will have:
\begin{equation}
0 \in z-x+t\partial h(z)
\end{equation}
After we rewite the function and replace $z$ by $u$ which is the optimal result from Proximal function:
\begin{equation}
{x-u \over t} \in \partial h(u)
\end{equation}
which means the calculated result from proximal function will be within the interval proportional to the subgradient of the simple-nonDerentiable function $h(x)$\\
\paragraph{(d)} From definition of Proximal operator, the optimal solution $x_{k+1}$ will be:
\begin{equation}
x_{k+1} = prox_{h,\alpha_k}(x_k - \alpha_k \nabla g(x_k) = x_k - \alpha_k \nabla g(x_k) - \alpha_k \partial h(x_{k+1})
\end{equation}
and from definition:
\begin{equation}
G_{\alpha_k}(x_k) = { x_k - prox_{h,\alpha_k}(x_k - \alpha_k \nabla g(x_k)) \over \alpha_k}
\end{equation}
after rewite:
\begin{equation}
x_k - \alpha_k \nabla g(x_k) - \alpha_k \partial h(x_{k+1}) = x_k - \alpha_k G_{\alpha_k}(x_k)
\end{equation}
Therefore
\begin{equation}
G_{\alpha_k}(x_k) - \nabla g(x_k) \in \partial h(x_{k+1})
\end{equation}
which is because h is not differentiable and the result will within the range of subgradient of $\partial h(x_{k+1})$

\end{document}






