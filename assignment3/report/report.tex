\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{mathtools}
\title{Deep Learning Assignment 3}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.
\author{
  Peter Yun-shao Sung
  \texttt{yss265@nyu.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle
\section{General Questions}
(a) Say if the first module is:
\begin{equation}
max(W_1X)
\end{equation}
where the W input layer maybe doing summation and summation just like matrix mutiplication does $WX$, and the $max$ function is a non-linear active function modifying the valuse like a neuron does before entering the next module:
\begin{equation}
W_2(max(W_2X))
\end{equation}
If now we don't have the active function then the formula will looks like:
\begin{equation}
W_2(W_1X) \to \bar{W}X
\end{equation}
which eventually all $W_i$ can become a single module $\bar{W}$

\section{Softmax regression gradient calculation}
Given
\begin{equation}
\hat{y} = \sigma (Wx+b) \textbf{ , where $x \in \mathbb{R}^d$,$W \in \mathbb{R}^{k\times d}$, $b \in \mathbb{R}^k$}
\end{equation}
where d is the input dimension, k is the number of classes, $\sigma$ is the softmax function:
\begin{equation}
\sigma(a)_i = {exp(a_i) \over \sum_j exp(a_j)}
\end{equation}
Which means a given input $x$ will output $y$ with probability of each class

\subsection{Derive ${\partial l\over \partial W_{ij}}$}
If the given cross-entropy loss defined as followed:
\begin{equation}
l(y, \hat{y}) = -\sum_i y_i\log\hat{y_i}
\end{equation}
As $W_{ij}$ will affect the prediction of class $i$ by multipling index $j$ in $x$, therefore we can derive:
\begin{equation}
{\partial l\over \partial W_{ij}} = {\partial l \over \partial \hat{y_i}} {\partial \hat{y_i} \over \partial W_{ij}}
\end{equation}
where:
\begin{equation}
l(y, \hat{y}) = -\sum_i y_i\log\hat{y_i} = -(y_i\log\hat{y_1} + y_2\log\hat{y_2} + \dots + y_i\log\hat{y_i} + \dots)
\end{equation}
and therefore
\begin{equation}
{\partial l \over \partial \hat{y_i}} = {-y_i\over \hat{y_i}}
\end{equation}
And we can rewrite for only for $\hat y_i$:
\begin{equation}
\hat{y_i} = {exp(a_i) \over \Sigma_j exp(a_j)} = {exp(a_i) \over C + exp(a_i)} \textbf{,  where $C = \sum_{k\neq i} exp(a_k)$}
\end{equation}
Since
\begin{equation}
{\partial exp(a_i) \over \partial W_{ij}} = X_jexp(a_i)
% {\partial exp(a_i) \over \partial W_{ij}} = W_{ij}exp(a_i)
\end{equation}
Therefore
\begin{equation}
{\partial \hat{y_i} \over \partial W_{ij}} = X_j\hat{y_i}(1-\hat{y_i})
% {\partial \hat{y_i} \over \partial W_{ij}} = W_{ij}\hat{y_i}(1-\hat{y_i})
\end{equation}
% \begin{equation}
% \hat{y_i} = {e^{(W_{ij}X_j+b_i)} \over C + e^{(W_{ij}X_j+b_i)} } \textbf{, where $C = \sum_{k\neq j} e^{W_{ik}X_k+b_i}$}
% \end{equation}
% \begin{equation}
% {\partial \hat{y_i} \over \partial W_{ij}} = {X_je^{(W_{ij}X_j+b_i)} \over C + e^{(W_{ij}X_j+b_i)} } - {X_je^{2(W_{ij}X_j+b_i)} \over (C + e^{(W_{ij}X_j+b_i)})^2} = X_j \hat{y_i} (1- \hat{y_i})
% \end{equation}
Finally, we will get the result of ${\partial l\over \partial W_{ij}}$:
\begin{equation}
{\partial l\over \partial W_{ij}} = {\partial l \over \partial \hat{y_i}} {\partial \hat{y_i} \over \partial W_{ij}} = -X_jy_i(1- \hat{y_i})
\end{equation}
\subsection{What happen when $y_{c_1}=1, \hat{y}_{c_2}=1, c_1 \neq c_2$}
\paragraph{(a)} This means something like $y = [1, 0, 0]^T$ and $\hat{y} = [0,0,1]^T$, and the predict is far different from true lable. This will cause the log part in loss (3) become negative infinity. We may not need to worry this because before one of the class predicted close to 1 and everything else close to 0, it will generate a great positive loss the the class that is miss-predicted trying to make the predict right to true label.
\section{Chain rule}
Without explicitly deriving the formula of $f(x, y)$, can we apply layers of functions to represent function $f$, which is similar to build deep learning architecture.
\begin{equation}
\begin{aligned}
f &= {{x^2+\sigma (y)}\over 3x + y - \sigma(x)} = {a \over b} \\
\implies {\partial f \over \partial x} &= {\partial a \over \partial x}{1\over b} - {a\over b^2}{\partial b \over \partial x} \\
\implies {\partial f \over \partial y} &= {\partial a \over \partial y}{1\over b} - {a\over b^2}{\partial b \over \partial y} \\
\implies {\partial a \over \partial x} &= 2x \\
\implies {\partial a \over \partial y} &= \sigma(y)(1-\sigma(y)) \\
\implies {\partial b \over \partial x} &= 3-\sigma(x)(1-\sigma(x)) \\
\implies {\partial b \over \partial y} &= 1 \\
\end{aligned}
\end{equation}

\paragraph{(b)} As $x = 1$ and $y = 0$, then for each of value from the function listed above:
\begin{equation}
\begin{aligned}
a &= 1+ \sigma(0) = 1.5 \\
b &= 3+ 0+ \sigma(1) = 2.269 \\
{\partial a \over \partial x} &= 2\cdot 1 = 2 \\
{\partial a \over \partial y} &= 0.5(1-0.5) = 0.25 \\
{\partial b \over \partial x} &= 3-0.731(1-0.731) = 2.803\\
{\partial b \over \partial y} &= 1 \\
\end{aligned}
\end{equation}
Therefore, applying each of the gradient at $(x,y) = (1,0)$ to the chain rule, we will get:

\begin{equation}
\begin{aligned}
{\partial f \over \partial x} &= {\partial a \over \partial x}{1\over b} - {a\over b^2}{\partial b \over \partial x} = 2\cdot{1\over2.269} - {1.5\over(2.269)^2}\cdot2.803 = 0.0647 \\
{\partial f \over \partial y} &= {\partial a \over \partial y}{1\over b} - {a\over b^2}{\partial b \over \partial y} =  0.25\cdot{1\over2.269} - {1.5 \over(2.269)^2}\cdot 1 =  -0.1811\\
\end{aligned}
\end{equation}

\section{Variants of pooling}
\section{Convolution}
\paragraph{(a)} As it is using 3x3 kernal along x and y axis of input, which is 5 and 5 respectively. The output of this layer will be $(5-3+1)\times(5-3+1)$ which is 3x3.\\
\paragraph{(b)} Assuming the kernel operation is point-point multiplication and summation, then the output of this layer is:\\
$\begin{pmatrix}
  109 & 92 & 72 \\[0.4em]
  108 & 85 & 74 \\[0.4em]
  110 & 74 & 79
\end{pmatrix}$ \\
\paragraph{(c)} \\
$\begin{pmatrix}
  4 & 7 & 10 & 6 & 3 \\[0.4em]
  9 & 17 & 25 & 16 & 8 \\[0.4em]
  11 & 23 & 34 & 23 & 11 \\[0.4em]
  7 & 16 & 24 & 17 & 8 \\[0.4em]
  2 & 6 & 9 &7 & 3
\end{pmatrix}$ \\


\section{Optimization}
\paragraph{(a)} say the encoder and decoder is defined as:
\begin{equation}
\begin{matrix}
z = W_1x + b_1 \\[0.4em]
\tilde{x} = W_2z + b_2
\end{matrix}
\end{equation}
And therefore the reconstruction loss $J$ will be:
\begin{equation}
J(W_1, b_1, W_2, b_2) = (\tilde{x}-x)^2 = (W_2(W_1x+b_1)+b_2-x)^2
\end{equation}

\paragraph{(b)} To have the gradient of reconstruction loss respective to the parameters, we take the derivative of each parameters:
\begin{equation}
\begin{matrix}
{\partial J \over \partial W_1} = W_2x \\[0.4em]
{\partial J \over \partial W_2} = W_1x + b_1
\end{matrix}
\end{equation}

\paragraph{(c)} Say now we are at stage $t$ and would like to compute $W_1^{t+1}$ and $W_2^{t+1}$:
\begin{equation}
\begin{matrix}
W_1^{t+1} = W_1^t - \mu_1^t {\partial J \over \partial W_1^t} = W_1^t - \mu_1^t(W_2x) \\[0.4em]
W_2^{t+1} = W_2^t - \mu_2^t {\partial J \over \partial W_2^t} = W_2^t - \mu_2^t(W_1x+b_1)
\end{matrix}
\end{equation}
where $\mu_1^t$ and $\mu_2^t$ are the step size at stage t

\paragraph{(d)} The updates during stochastic gradient descent usually involves Move-Forward and Correction stages and this oscillation may delay the efficiency of convergence, and therefore adding a momentum term may make the update toward the good direction as well as with the previous update history considered:
\begin{equation}
\begin{matrix}
W_1^{t+1} = W_1^t - \mu_1^t {\partial J \over \partial W_1^t} + \Delta W_1^t \\[0.4em]
W_2^{t+1} = W_2^t - \mu_2^t {\partial J \over \partial W_2^t} + \Delta W_2^t
\end{matrix}
\end{equation}

\section{Top-k error}
For image classification, sometime the class is ambiguous, and the loss during is being modified to consider multiple label. The top-k error rate is the fraction of test images for which the correct label is not among the top-k labels considered most probable. The reason why ImageNet using both top-5 and top-1 is due to sometimes only looking at top-1 error cannot be objective enought to evaluate the model because the image itself contains multi-label, and therefore evaluating top-5 error is important too.
\section{t-SNE}
\section{Proximal gradient descent}
\paragraph{(a)} Since Proximal operator is defined as:
\begin{equation}
prox_{h,t}(x) = argmin_z{1\over2}||z-x||_2^2 + th(z)
\end{equation}
which the optimal condition is to have the gradient w.r.t $z$ equal to 0:
\begin{equation}
0 \in z-x+t\partial h(z)
\end{equation}
if function $h(z)=||z||_1$ and $z_i \neq 0$, then:
\begin{equation}
\partial h(z) = sign(z)
\end{equation}
And therefore the optimal solution $z^\ast$ will be:
\begin{equation}
z^\ast = x - t\cdot sign(z^\ast)
\end{equation}
Noted that if $z_i^\ast<0$, then $x_i<-t$, and if $z_i^\ast>0$, then $x_i>t$. This implies $|x_i|>t$ and $sign(z_i^\ast) = sign(x_i)$, and we can rewrite formula to:
\begin{equation}
z_i^\ast = x_i - t\cdot sign(x_i)
\end{equation}
Then if the solution $z_i^\ast=0$, the subgradient of l1-norm is in the interval of [-1, 1], and we can write:
\begin{equation}
0 \in -x_i + t \cdot [-1,1] \implies x_i \in [-t, t] \implies |x_i| \leq t
\end{equation}
Therefore the solution of Proximal operator will be:
\begin{equation}
z_i^\ast = \begin{cases}
    0       & \quad \text{if } |x_i|\leq t\\
    x_i - t \cdot sign(x_i)  & \quad \text{if } |x_i|>t\\
  \end{cases}
\end{equation}
which is
\begin{equation}
prox_{h,t}(x) = S_t(x) = (|x|-t)_+\odot sign(x) \quad \text{(element-wise)}
\end{equation}
which is a soft-threshold fuction with t as threshold value\\
\paragraph{(b)} In the field of signal processing, the true signal usually will be blurred as followed:
\begin{equation}
Ax = b
\end{equation}
where $A$ is the blur operation, b is the known observed blured-signal. The way to solve true signal $x$ is called deblurring problem:
\begin{equation}
min_x\{F(x) \equiv {1\over 2}||b-Ax||_2^2 + \lambda||x||_1\}
\end{equation}
This is ISTA problem, and as we can see the first term is convex and differentiable, and the second term is convex and simple l1-norm function. Then the ISTA is become one example of proximal gradient descent\\
\paragraph{(c)} From the definition of Proximal operator the optimal solution is where ${\partial prox_{h,t}\over \partial z} = 0$, and therefore we will have:
\begin{equation}
0 \in z-x+t\partial h(z)
\end{equation}
After we rewite the function and replace $z$ by $u$ which is the optimal result from Proximal function:
\begin{equation}
{x-u \over t} \in \partial h(u)
\end{equation}
which means the calculated result from proximal function will be within the interval proportional to the subgradient of the simple-nonDerentiable function $h(x)$\\
\paragraph{(d)} From definition of Proximal operator, the optimal solution $x_{k+1}$ will be:
\begin{equation}
x_{k+1} = prox_{h,\alpha_k}(x_k - \alpha_k \nabla g(x_k) = x_k - \alpha_k \nabla g(x_k) - \alpha_k \partial h(x_{k+1})
\end{equation}
and from definition:
\begin{equation}
G_{\alpha_k}(x_k) = { x_k - prox_{h,\alpha_k}(x_k - \alpha_k \nabla g(x_k)) \over \alpha_k}
\end{equation}
after rewite:
\begin{equation}
x_k - \alpha_k \nabla g(x_k) - \alpha_k \partial h(x_{k+1}) = x_k - \alpha_k G_{\alpha_k}(x_k)
\end{equation}
Therefore
\begin{equation}
G_{\alpha_k}(x_k) - \nabla g(x_k) \in \partial h(x_{k+1})
\end{equation}
which is because h is not differentiable and the result will within the range of subgradient of $\partial h(x_{k+1})$

\end{document}






