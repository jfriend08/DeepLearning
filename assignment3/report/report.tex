\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{mathtools}
\title{Deep Learning Assignment 3}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.
\author{
  Peter Yun-shao Sung
  \texttt{yss265@nyu.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle
\section{General Questions}
(a) Say if the first module is:
\begin{equation}
max(W_1X)
\end{equation}
where the W input layer maybe doing summation and summation just like matrix mutiplication does $WX$, and the $max$ function is a non-linear active function modifying the valuse like a neuron does before entering the next module:
\begin{equation}
W_2(max(W_2X))
\end{equation}
If now we don't have the active function then the formula will looks like:
\begin{equation}
W_2(W_1X) \to \bar{W}X
\end{equation}
which eventually all $W_i$ can become a single module $\bar{W}$

\section{Softmax regression gradient calculation}
Given
\begin{equation}
\hat{y} = \sigma (Wx+b) \textbf{ , where $x \in \mathbb{R}^d$,$W \in \mathbb{R}^{k\times d}$, $b \in \mathbb{R}^k$}
\end{equation}
where d is the input dimension, k is the number of classes, $\sigma$ is the softmax function:
\begin{equation}
\sigma(a)_i = {exp(a_i) \over \sum_j exp(a_j)}
\end{equation}
Which means a given input $x$ will output $y$ with probability of each class

\subsection{Derive ${\partial l\over \partial W_{ij}}$}
If the given cross-entropy loss defined as followed:
\begin{equation}
l(y, \hat{y}) = -\sum_i y_i\log\hat{y_i}
\end{equation}
As $W_{ij}$ will affect the prediction of class $i$ by multipling index $j$ in $x$, therefore we can derive:
\begin{equation}
{\partial l\over \partial W_{ij}} = {\partial l \over \partial \hat{y_i}} {\partial \hat{y_i} \over \partial W_{ij}}
\end{equation}
where:
\begin{equation}
l(y, \hat{y}) = -\sum_i y_i\log\hat{y_i} = -(y_i\log\hat{y_1} + y_2\log\hat{y_2} + \dots + y_i\log\hat{y_i} + \dots)
\end{equation}
and therefore
\begin{equation}
{\partial l \over \partial \hat{y_i}} = {-y_i\over \hat{y_i}}
\end{equation}
And we can rewrite for only for $\hat y_i$:
\begin{equation}
\hat{y_i} = {exp(a_i) \over \Sigma_j exp(a_j)} = {exp(a_i) \over C + exp(a_i)} \textbf{,  where $C = \sum_{k\neq i} exp(a_k)$}
\end{equation}
Since
\begin{equation}
{\partial exp(a_i) \over \partial W_{ij}} = W_{ij}exp(a_i)
\end{equation}
Therefore
\begin{equation}
{\partial \hat{y_i} \over \partial W_{ij}} = W_{ij}\hat{y_i}(1-\hat{y_i})
\end{equation}
% \begin{equation}
% \hat{y_i} = {e^{(W_{ij}X_j+b_i)} \over C + e^{(W_{ij}X_j+b_i)} } \textbf{, where $C = \sum_{k\neq j} e^{W_{ik}X_k+b_i}$}
% \end{equation}
% \begin{equation}
% {\partial \hat{y_i} \over \partial W_{ij}} = {X_je^{(W_{ij}X_j+b_i)} \over C + e^{(W_{ij}X_j+b_i)} } - {X_je^{2(W_{ij}X_j+b_i)} \over (C + e^{(W_{ij}X_j+b_i)})^2} = X_j \hat{y_i} (1- \hat{y_i})
% \end{equation}
Finally, we will get the result of ${\partial l\over \partial W_{ij}}$:
\begin{equation}
{\partial l\over \partial W_{ij}} = {\partial l \over \partial \hat{y_i}} {\partial \hat{y_i} \over \partial W_{ij}} = -X_jy_i(1- \hat{y_i})
\end{equation}
\subsection{What happen when $y_{c_1}=1, \hat{y}_{c_2}=1, c_1 \neq c_2$}
This means something like $y = [1, 0, 0]^T$ and $\hat{y} = [0,0,1]^T$, and the predict is far different from true lable. This will cause the log part in loss (3) become negative infinity. We may not need to worry this because before one of the class predicted close to 1 and everything else close to 0, it will generate a great positive loss the the class that is miss-predicted trying to make the predict right to true label.
\section{Chain rule}
\section{Variants of pooling}
\section{Convolution}
(a) As it is using 3x3 kernal along x and y axis of input, which is 5 and 5 respectively. The output of this layer will be $(5-3+1)\times(5-3+1)$ which is 3x3.\\
(b) Assuming the kernel operation is point-point multiplication and summation, then the output of this layer is:\\
$\begin{pmatrix}
  109 & 92 & 72 \\[0.4em]
  108 & 85 & 74 \\[0.4em]
  110 & 74 & 79
\end{pmatrix}$ \\
(c)
$\begin{pmatrix}
  4 & 7 & 10 & 6 & 3 \\[0.4em]
  9 & 17 & 25 & 16 & 8 \\[0.4em]
  11 & 23 & 34 & 23 & 11 \\[0.4em]
  7 & 16 & 24 & 17 & 8 \\[0.4em]
  2 & 6 & 9 &7 & 3
\end{pmatrix}$ \\


\section{Optimization}
\section{Top-k error}
\section{t-SNE}
\section{Proximal gradient descent}
(a) Since Proximal operator is defined as:
\begin{equation}
prox_{h,t}(x) = argmin_z{1\over2}||z-x||_2^2 + th(z)
\end{equation}
which the optimal condition is to have the gradient w.r.t $z$ equal to 0:
\begin{equation}
0 \in z-x+t\partial h(z)
\end{equation}
if function $h(z)=||z||_1$ and $z_i \neq 0$, then:
\begin{equation}
\partial h(z) = sign(x)
\end{equation}
And therefore the optimal solution $z^\ast$ will be:
\begin{equation}
z^\ast = x - t\cdot sign(z^\ast)
\end{equation}
Noted that if $z_i^\ast<0$, then $x_i<-\lambda$, and if $z_i^\ast>0$, then $x_i>\lambda$. This implies $|x_i|>\lambda$ and $sign(z_i^\ast) = sign(x_i)$, and we can rewrite formula to:
\begin{equation}
z_i^\ast = x_i - t\cdot sign(x_i)
\end{equation}
Then if the solution $z_i^\ast=0$, the subgradient of l1-norm is in the interval of [-1, 1], and we can write:
\begin{equation}
0 \in -x_i + t \cdot [-1,1] \implies x_i \in [-t, t] \implies |x_i| \leq t
\end{equation}
Therefore the solution of Proximal operator will be:
\begin{equation}
z_i^\ast = \begin{cases}
    0       & \quad \text{if } |x_i|\leq t\\
    x_i-t \cdot sign(x_i)  & \quad \text{if } |x_i|>t\\
  \end{cases}
\end{equation}
which is
\begin{equation}
prox_{h,t}(x) = S_t(x) = (|x|-t)_+\odot sign(x) \quad \text{(element-wise)}
\end{equation}
which is a soft-threshold fuction with t as threshold value\\
(b) In the field of signal processing, the true signal usually will be blurred as followed:
\begin{equation}
Ax = b
\end{equation}
where $A$ is the blur operation, b is the known observed blured-signal. The way to solve true signal $x$ is called deblurring problem:
\begin{equation}
min_x\{F(x) \equiv {1\over 2}||b-Ax||_2^2 + \lambda||x||_1\}
\end{equation}
This is ISTA problem, and as we can see the first term is convex and differentiable, and the second term is convex and simple l1-norm function. Then the ISTA is become one example of proximal gradient descent



\end{document}






